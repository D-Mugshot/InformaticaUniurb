{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection Neural Network\n",
    "Questo notebook ha l'obiettivo di sviluppare un modello di machine learning per la rilevazione delle frodi. L'obiettivo principale è identificare transazioni potenzialmente fraudolente all'interno di un dataset, addestrando un modello predittivo che possa distinguere tra transazioni lecite e fraudolente. Utilizzeremo diverse tecniche di machine learning per confrontare le loro prestazioni e scegliere la più efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup delle librerie\n",
    "In questa sezione vengono importate e installate le librerie necessarie per l'analisi dei dati e la costruzione del modello di machine learning. Le librerie includono strumenti per la manipolazione dei dati (pandas, numpy), la visualizzazione (matplotlib, seaborn), l'apprendimento automatico (scikit-learn, tensorflow, imblearn), e il salvataggio del modello (joblib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pinguiz/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n",
      "2024-08-26 17:38:26.993016: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-26 17:38:27.067099: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-26 17:38:27.161848: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-26 17:38:27.237912: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-26 17:38:27.257657: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-26 17:38:27.405418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-26 17:38:28.445015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn',\n",
    "    'tensorflow',\n",
    "    'pickle',\n",
    "    'imblearn',\n",
    "    'plotly',\n",
    "    'pydot'\n",
    "]\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "install_packages(packages)\n",
    "\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from datetime import date, datetime\n",
    "\n",
    "!pip install -q kaggle\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import plotly.offline as pyo\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmmEpxmtOjv3"
   },
   "source": [
    "## Dataset Download\n",
    "In questa sezione viene definito il percorso del dataset e viene scaricato un file ZIP che contiene i dati delle transazioni con carte di credito. Il dataset viene poi estratto e caricato in un DataFrame di pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HayrmBdOEGD3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403 - Forbidden - Permission 'datasets.get' was denied\n",
      "Error: Downloaded file is not a zip file or download failed.\n",
      "Failed to load credit cards data.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "BASE_PATH = './'\n",
    "PROJECT_PATH = os.path.join(BASE_PATH, '')\n",
    "DATASET_DIR = os.path.join(PROJECT_PATH, 'datasets/')\n",
    "ZIP_FILE_PATH = os.path.join(DATASET_DIR, 'credit-card-transaction-records-dataset.zip')\n",
    "CSV_FILE_PATH = os.path.join(DATASET_DIR, 'credit_card_purchases.csv')\n",
    "\n",
    "CMAP = 'Blues'\n",
    "\n",
    "\n",
    "def setup_kaggle():\n",
    "    kaggle_json_path = os.path.join(PROJECT_PATH, 'kaggle.json')\n",
    "\n",
    "    if not os.path.exists(kaggle_json_path):\n",
    "        print(\"kaggle.json not found.\")\n",
    "        return False\n",
    "\n",
    "    kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
    "    os.makedirs(kaggle_dir, exist_ok=True)\n",
    "\n",
    "    os.system(f'cp \"{kaggle_json_path}\" \"{kaggle_dir}/kaggle.json\"')\n",
    "    os.system(f'chmod 600 \"{kaggle_dir}/kaggle.json\"')\n",
    "\n",
    "    return True\n",
    "\n",
    "def download_data():\n",
    "    if not os.path.isfile(ZIP_FILE_PATH):\n",
    "        Path(DATASET_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if not setup_kaggle():\n",
    "            return None\n",
    "\n",
    "        !kaggle datasets download -d muhammadehsan000/credit-card-transaction-records-dataset -p \"{DATASET_DIR}\" --force\n",
    "\n",
    "        if not os.path.isfile(ZIP_FILE_PATH) or not ZIP_FILE_PATH.endswith('.zip'):\n",
    "            print(\"Error: Downloaded file is not a zip file or download failed.\")\n",
    "            return None\n",
    "\n",
    "        with zipfile.ZipFile(ZIP_FILE_PATH, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATASET_DIR)\n",
    "\n",
    "    if not os.path.isfile(CSV_FILE_PATH):\n",
    "        return None\n",
    "\n",
    "    return pd.read_csv(CSV_FILE_PATH)\n",
    "\n",
    "cc_purchases = download_data()\n",
    "if cc_purchases is None:\n",
    "    print(\"Failed to load credit cards data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOeRkNEEOqj8"
   },
   "source": [
    "## Esplorazione e visualizzazione dei dati\n",
    "Una volta caricato il dataset, si procede con una rapida ispezione dei dati per comprendere la struttura del dataset, verificare la presenza di eventuali valori mancanti e osservare statistiche descrittive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 878
    },
    "id": "4PQBKrul-12P",
    "outputId": "2516cc00-8882-482f-841b-996f062cc413"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcc_purchases\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "print(cc_purchases.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cc_purchases.info())\n",
    "print(cc_purchases.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_purchases['is_fraud'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Percentuale di Transazioni Fraudolente')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "sns.despine(f)\n",
    "sns.histplot(cc_purchases['amt'], bins=50, kde=True, log_scale=True)\n",
    "ax.xaxis.set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "plt.title('Distribuzione degli Importi delle Transazioni')\n",
    "plt.xlabel('Importo ($)')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='long', y='lat', hue='is_fraud', data=cc_purchases, palette='coolwarm', alpha=0.6)\n",
    "plt.title('Distribuzione Geografica delle Transazioni (Fraudolente vs Non Fraudolente)')\n",
    "plt.show()\n",
    "plt.savefig(os.path.join(PROJECT_PATH, '3.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.countplot(y='category', hue='is_fraud', data=cc_purchases)\n",
    "plt.title('Numero di Transazioni per Categoria (Fraudolente vs Non Fraudolente)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsU8_EGeXFX5"
   },
   "source": [
    "## Preprocessing dei Dati\n",
    "In questa sezione, vengono eseguite diverse operazioni di preprocessing per preparare i dati all'addestramento del modello. Questo include la gestione delle variabili temporali, la rimozione di colonne non necessarie, e la trasformazione di variabili categoriali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_age(dob):\n",
    "  if isinstance(dob, str):\n",
    "    dob = datetime.strptime(dob, '%Y-%m-%d').date() \n",
    "\n",
    "  today = date.today()\n",
    "  age = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "  return age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 635
    },
    "id": "nwvSZ2YPwykY",
    "outputId": "3d25cf84-e5ec-48bc-947a-2af87e6c5501"
   },
   "outputs": [],
   "source": [
    "cc_purchases_df2 = cc_purchases.copy()\n",
    "\n",
    "TARGET_COL = 'is_fraud'\n",
    "TARGET = cc_purchases_df2[TARGET_COL]\n",
    "\n",
    "# Drop extra columns\n",
    "cc_purchases_df2 = cc_purchases.drop(columns=['Unnamed: 0','trans_date_trans_time','cc_num','merchant','first','last','street','city','state','trans_num','merch_zipcode'])\n",
    "\n",
    "# Replace date of birth with age\n",
    "cc_purchases_df2['age'] = cc_purchases_df2['dob'].apply(calculate_age)\n",
    "cc_purchases_df2 = cc_purchases_df2.drop(columns=['dob'])\n",
    "\n",
    "# Order the dataframe\n",
    "new_column_order = ['unix_time','lat','long', 'amt', 'category', 'gender', 'age', 'job', 'zip', 'city_pop', 'merch_lat', 'merch_long', 'is_fraud']\n",
    "cc_purchases_df2 = cc_purchases_df2.reindex(columns=new_column_order)\n",
    "\n",
    "cc_purchases_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "ImHEfLaINPPt",
    "outputId": "809e8cbb-95a6-41b8-f079-a4ba31d28be4"
   },
   "outputs": [],
   "source": [
    "def report(dfs, names):\n",
    "\n",
    "    rep = pd.DataFrame(dfs[0].dtypes, columns=['dtypes'])\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        rep[f\"{names[i]}_missing#\"] = dfs[i].isna().sum()\n",
    "        rep[f\"{names[i]}_missing%\"] = (dfs[i].isna().sum())/len(dfs[i])\n",
    "        rep[f\"{names[i]}_uniques\"] = dfs[i].nunique().values\n",
    "\n",
    "    return rep\n",
    " \n",
    "names = [\"Credit Cards Dataframe\"]\n",
    "rep = report([cc_purchases_df2], names)\n",
    "rep.style.background_gradient(cmap=CMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_purchases_df3 = cc_purchases_df2.copy()\n",
    "\n",
    "cat_cols = ['category', 'gender', 'job']\n",
    "\n",
    "encoders = {}\n",
    "\n",
    "# Encode categorical columns\n",
    "for col in cat_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    cc_purchases_df3[col] = encoder.fit_transform(cc_purchases_df3[col])\n",
    "    encoders[col] = encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 771
    },
    "id": "KwB2RNTJXGq5",
    "outputId": "c3f7c01d-0db9-4789-b0ee-479b4b05d048"
   },
   "outputs": [],
   "source": [
    "corr = cc_purchases_df3.corr()\n",
    "mask = np.triu(np.ones_like(corr))\n",
    "plt.figure(figsize = (15,9))\n",
    "sns.heatmap(corr,annot = True, cmap=CMAP, mask=mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_purchases_df3 = cc_purchases_df3.drop(['zip','merch_long','merch_lat'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "LPevccY8xnbx",
    "outputId": "2e1145e8-18b5-47c8-f07e-6b79360a5769"
   },
   "outputs": [],
   "source": [
    "corr = cc_purchases_df3.corr()\n",
    "mask = np.triu(np.ones_like(corr))\n",
    "plt.figure(figsize = (15,9))\n",
    "sns.heatmap(corr,annot = True, cmap=CMAP, mask=mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_purchases_df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgbhnGyueOMx"
   },
   "source": [
    "## Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8i64XcJe5pCZ"
   },
   "outputs": [],
   "source": [
    "#MODEL = 'model.pkl'\n",
    "MODEL = None\n",
    "\n",
    "# Data preparation without SMOTE\n",
    "X = cc_purchases_df3.drop('is_fraud', axis=1)\n",
    "y = cc_purchases_df3['is_fraud']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(zip(np.unique(y_train),class_weights))\n",
    "# Compute sample weights\n",
    "sample_weights_train = compute_sample_weight(class_weight=class_weights_dict, y=y_train)\n",
    "\n",
    "\n",
    "print(f\"Class Weights: {class_weights_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxPM2hQVLMdf"
   },
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_out):\n",
    "    out_classes = n_out  \n",
    "    HIDDEN1 = 64  \n",
    "    HIDDEN2 = 128  \n",
    "\n",
    "    print(f\"Making model with input shape {input_shape}, hidden {HIDDEN1}, {HIDDEN2}, out classes {out_classes}\")\n",
    "    \n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Input(shape=input_shape))\n",
    "\n",
    "    # First Conv1D layer\n",
    "    model.add(keras.layers.Conv1D(filters=HIDDEN1, kernel_size=5, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "    # Second Conv1D layer (optional, can be removed or adjusted)\n",
    "    model.add(keras.layers.Conv1D(filters=HIDDEN1, kernel_size=3, padding='same'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.ReLU())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "    # LSTM layer\n",
    "    model.add(keras.layers.LSTM(HIDDEN2))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(out_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL is None:\n",
    "    input_shape = X_train_scaled.shape[1]\n",
    "    model = create_model(input_shape)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train, \n",
    "        epochs=100, \n",
    "        batch_size=32, \n",
    "        validation_data=(X_val_scaled, y_val), \n",
    "        sample_weight=sample_weights_train,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    with open('model.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "    \n",
    "    history_dict = history.history\n",
    "    with open('history.json', 'w') as file:\n",
    "        json.dump(history_dict, file)\n",
    "else:\n",
    "    with open('model.pkl', 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    with open('history.json', 'r') as file:\n",
    "        history_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yzgDNE9k6Q8S",
    "outputId": "97688a4e-c2ac-4e16-984d-5678206ce06c"
   },
   "outputs": [],
   "source": [
    "# Predict probabilities on the validation set\n",
    "y_val_prob = model.predict(X_val_scaled)\n",
    "\n",
    "# Calcola la curva Precision-Recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_val, y_val_prob)\n",
    "\n",
    "# Trova la soglia che massimizza l'F1-score\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
    "best_threshold = thresholds[f1_scores.argmax()]\n",
    "\n",
    "print(f'Optimal threshold: {best_threshold}')\n",
    "\n",
    "# Converti le probabilità in etichette di classe usando la soglia ottimale\n",
    "y_val_pred = (y_val_prob > best_threshold).astype(int)\n",
    "\n",
    "# Stampa il report di classificazione con la soglia ottimale\n",
    "report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose().round(3)\n",
    "\n",
    "# Creare la figura\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Imposta la dimensione della figura\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "the_table = ax.table(cellText=report_df.values, \n",
    "                     colLabels=report_df.columns, \n",
    "                     rowLabels=report_df.index,\n",
    "                     cellLoc='center', \n",
    "                     loc='center')\n",
    "\n",
    "# Salva l'immagine (opzionale)\n",
    "plt.savefig('classification_report.png')\n",
    "\n",
    "# Mostra l'immagine\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xb4e8wQSN9QU"
   },
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, rankdir='TB', expand_nested=True, dpi=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKMNLk17ZCU3"
   },
   "outputs": [],
   "source": [
    "# Plotting training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plotting training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkO-JaVqlYV7"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Plot the confusion matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g',\n",
    "            xticklabels=cc_purchases_df3[TARGET_COL].unique(),\n",
    "            yticklabels=cc_purchases_df3[TARGET_COL].unique())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
